{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Daniel Vidal, Neil de la Fuente, Joan Samper**"
      ],
      "metadata": {
        "id": "Dh3plDZbkCUR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noSoe2Bfwt5U"
      },
      "source": [
        "# Exercise 1: N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJHSuILGwt5W",
        "outputId": "074ff6b0-a86a-4121-c0c8-e95b5d008bf5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/samper12/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/samper12/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb8YSY42wt5X"
      },
      "source": [
        "### 1. Consider the sentence “To Sherlock Holmes she is always ‘The Woman.’ I have seldom heard him mention her under any other name.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy5h3oDvwt5Y"
      },
      "outputs": [],
      "source": [
        "sentence = \"To Sherlock Holmes she is always 'The Woman' I have seldom heard him mention her under any other name.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5luVvWZPwt5Z"
      },
      "source": [
        "### 2. Tokenize the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFGX9Showt5Z",
        "outputId": "0fc0b100-5ae1-45b9-8d20-b07715aba7ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['To',\n",
              " 'Sherlock',\n",
              " 'Holmes',\n",
              " 'she',\n",
              " 'is',\n",
              " 'always',\n",
              " \"'The\",\n",
              " 'Woman',\n",
              " \"'\",\n",
              " 'I',\n",
              " 'have',\n",
              " 'seldom',\n",
              " 'heard',\n",
              " 'him',\n",
              " 'mention',\n",
              " 'her',\n",
              " 'under',\n",
              " 'any',\n",
              " 'other',\n",
              " 'name',\n",
              " '.']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = nltk.word_tokenize(sentence)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH5rZiMkwt5a"
      },
      "source": [
        "### 3. Calculate the n-grams of the sentence, with n = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiIGLCurwt5b",
        "outputId": "e3a69c51-7552-48b1-d184-a2d5ee197db2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('To', 'Sherlock'),\n",
              " ('Sherlock', 'Holmes'),\n",
              " ('Holmes', 'she'),\n",
              " ('she', 'is'),\n",
              " ('is', 'always'),\n",
              " ('always', \"'The\"),\n",
              " (\"'The\", 'Woman'),\n",
              " ('Woman', \"'\"),\n",
              " (\"'\", 'I'),\n",
              " ('I', 'have'),\n",
              " ('have', 'seldom'),\n",
              " ('seldom', 'heard'),\n",
              " ('heard', 'him'),\n",
              " ('him', 'mention'),\n",
              " ('mention', 'her'),\n",
              " ('her', 'under'),\n",
              " ('under', 'any'),\n",
              " ('any', 'other'),\n",
              " ('other', 'name'),\n",
              " ('name', '.')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigrams = list(nltk.bigrams(tokens))\n",
        "bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuh41_ZTwt5c",
        "outputId": "73029f1b-964c-4c21-f5e0-dbfc682a6dc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('To', 'Sherlock', 'Holmes'),\n",
              " ('Sherlock', 'Holmes', 'she'),\n",
              " ('Holmes', 'she', 'is'),\n",
              " ('she', 'is', 'always'),\n",
              " ('is', 'always', \"'The\"),\n",
              " ('always', \"'The\", 'Woman'),\n",
              " (\"'The\", 'Woman', \"'\"),\n",
              " ('Woman', \"'\", 'I'),\n",
              " (\"'\", 'I', 'have'),\n",
              " ('I', 'have', 'seldom'),\n",
              " ('have', 'seldom', 'heard'),\n",
              " ('seldom', 'heard', 'him'),\n",
              " ('heard', 'him', 'mention'),\n",
              " ('him', 'mention', 'her'),\n",
              " ('mention', 'her', 'under'),\n",
              " ('her', 'under', 'any'),\n",
              " ('under', 'any', 'other'),\n",
              " ('any', 'other', 'name'),\n",
              " ('other', 'name', '.')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trigrams = list(nltk.trigrams(tokens))\n",
        "trigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6fzzeGFwt5e"
      },
      "source": [
        "### 4. Perform text pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TSxZsC9wt5f"
      },
      "source": [
        "1. Remove special characters such as *,<,>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gr4ubbMwt5f",
        "outputId": "9ad2f7b8-d48d-469c-ef78-3380a34440a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', \"'The\", 'Woman', \"'\", 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.']\n"
          ]
        }
      ],
      "source": [
        "#Remove all the tokens that are equal to \"*\", \">\" or \"<\"\n",
        "prep_tok = [word for word in tokens if word not in [\"*\", \">\", \"<\"]]\n",
        "print(prep_tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eNKdp2Hwt5g"
      },
      "source": [
        "2. Remove punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26A7ZyVNwt5h",
        "outputId": "86cf9f2d-e7f1-43e9-8027-7cc4c82b1956"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['To',\n",
              " 'Sherlock',\n",
              " 'Holmes',\n",
              " 'she',\n",
              " 'is',\n",
              " 'always',\n",
              " \"'The\",\n",
              " 'Woman',\n",
              " 'I',\n",
              " 'have',\n",
              " 'seldom',\n",
              " 'heard',\n",
              " 'him',\n",
              " 'mention',\n",
              " 'her',\n",
              " 'under',\n",
              " 'any',\n",
              " 'other',\n",
              " 'name']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Remove punctuation as well\n",
        "import string\n",
        "preprocessed_tokens = [word for word in prep_tok if word not in string.punctuation]\n",
        "preprocessed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_CWzAD1wt5h"
      },
      "source": [
        "### 5. Compute the frequency of each n-gram, and show the top 10 bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X4o_3-Awt5h",
        "outputId": "5cb6a5f9-eff7-4caa-ad15-96d13da466e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(('To', 'Sherlock'), 1), (('Sherlock', 'Holmes'), 1), (('Holmes', 'she'), 1), (('she', 'is'), 1), (('is', 'always'), 1), (('always', \"'The\"), 1), ((\"'The\", 'Woman'), 1), (('Woman', 'I'), 1), (('I', 'have'), 1), (('have', 'seldom'), 1)]\n",
            "[(('To', 'Sherlock', 'Holmes'), 1), (('Sherlock', 'Holmes', 'she'), 1), (('Holmes', 'she', 'is'), 1), (('she', 'is', 'always'), 1), (('is', 'always', \"'The\"), 1), (('always', \"'The\", 'Woman'), 1), ((\"'The\", 'Woman', 'I'), 1), (('Woman', 'I', 'have'), 1), (('I', 'have', 'seldom'), 1), (('have', 'seldom', 'heard'), 1)]\n",
            "[(('To', 'Sherlock', 'Holmes', 'she'), 1), (('Sherlock', 'Holmes', 'she', 'is'), 1), (('Holmes', 'she', 'is', 'always'), 1), (('she', 'is', 'always', \"'The\"), 1), (('is', 'always', \"'The\", 'Woman'), 1), (('always', \"'The\", 'Woman', 'I'), 1), ((\"'The\", 'Woman', 'I', 'have'), 1), (('Woman', 'I', 'have', 'seldom'), 1), (('I', 'have', 'seldom', 'heard'), 1), (('have', 'seldom', 'heard', 'him'), 1)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "bigram_counts = Counter(list(nltk.bigrams(preprocessed_tokens)))\n",
        "print(bigram_counts.most_common(10))\n",
        "trigram_counts = Counter(list(nltk.trigrams(preprocessed_tokens)))\n",
        "print(trigram_counts.most_common(10))\n",
        "fourgram_counts = Counter(nltk.ngrams(preprocessed_tokens, 4))\n",
        "print(fourgram_counts.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXLIkIC_wt5i"
      },
      "source": [
        "Now Let's do the same with the **Moby Dick** text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s35__QS0wt5i"
      },
      "outputs": [],
      "source": [
        "with open(\"contents/moby_dick.txt\", 'r') as file:\n",
        "    mobydick = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDg9rAstwt5j",
        "outputId": "f9fd9560-e684-44dc-8ce5-fa83a0063ffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in Moby Dick: 259422\n"
          ]
        }
      ],
      "source": [
        "mobydick_tokens = nltk.word_tokenize(mobydick)\n",
        "print(\"Number of tokens in Moby Dick:\", len(mobydick_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTA3WBkpwt5j"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "#Now remove punctuations and \"*\", \"<\", \">\" signs\n",
        "prep_mobydick_tokens = [word for word in mobydick_tokens if word not in string.punctuation and word not in [\"*\", \"<\", \">\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bqsECS6wt5k"
      },
      "source": [
        "Compute the ngram rankings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGzSCG4twt5k",
        "outputId": "938032ba-e0c5-43c4-95c3-c4486058cce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most common bigrams are:\n",
            "[(('of', 'the'), 1895), (('’', 's'), 1790), (('in', 'the'), 1139), (('to', 'the'), 732), (('”', '“'), 494), (('from', 'the'), 437), (('of', 'his'), 371), (('and', 'the'), 364), (('of', 'a'), 337), (('on', 'the'), 337)]\n",
            "\n",
            "The most common trigrams are:\n",
            "[(('don', '’', 't'), 95), (('of', 'the', 'whale'), 88), (('whale', '’', 's'), 80), (('the', 'Sperm', 'Whale'), 77), (('ship', '’', 's'), 77), (('Ahab', '’', 's'), 76), (('’', 's', 'the'), 74), (('he', '’', 's'), 74), (('’', 's', 'a'), 73), (('it', '’', 's'), 63)]\n",
            "\n",
            "The most common fourgrams are:\n",
            "[(('the', 'ship', '’', 's'), 52), (('the', 'whale', '’', 's'), 48), (('the', 'Pequod', '’', 's'), 45), (('I', 'don', '’', 't'), 36), (('of', 'the', 'Sperm', 'Whale'), 30), (('”', 'said', 'I', '“'), 28), (('Sperm', 'Whale', '’', 's'), 24), (('at', 'the', 'same', 'time'), 20), (('old', 'man', '’', 's'), 19), (('of', 'the', 'whale', '’'), 17)]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "bigram_counts = Counter(list(nltk.bigrams(prep_mobydick_tokens)))\n",
        "print(f\"The most common bigrams are:\\n{bigram_counts.most_common(10)}\\n\")\n",
        "trigram_counts = Counter(list(nltk.trigrams(prep_mobydick_tokens)))\n",
        "print(f\"The most common trigrams are:\\n{trigram_counts.most_common(10)}\\n\")\n",
        "fourgram_counts = Counter(nltk.ngrams(prep_mobydick_tokens, 4))\n",
        "print(f\"The most common fourgrams are:\\n{fourgram_counts.most_common(10)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLkh_oAdkBc4"
      },
      "outputs": [],
      "source": [
        "bigrams = list(nltk.bigrams(prep_mobydick_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiueksiAkBc5",
        "outputId": "5a3aead9-715c-412f-e1aa-cbdc34eb7437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diversity: 0.555519997155328\n"
          ]
        }
      ],
      "source": [
        "# N-gram Diversity\n",
        "diversity = len(set(bigrams)) / len(bigrams)\n",
        "print('diversity:',diversity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq_AUv4pkBc5",
        "outputId": "9860b200-0811-4c32-d1c4-cd83579a581a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('\\ufeff', 'The'),\n",
              " ('Gutenberg', 'EBook'),\n",
              " ('EBook', 'of'),\n",
              " ('Melville', 'This'),\n",
              " ('www.gutenberg.org', 'Title'),\n",
              " ('Title', 'Moby'),\n",
              " ('Whale', 'Author'),\n",
              " ('Author', 'Herman'),\n",
              " ('Melville', 'Release'),\n",
              " ('Release', 'Date'),\n",
              " ('Date', 'December'),\n",
              " ('December', '25'),\n",
              " ('25', '2008'),\n",
              " ('2008', 'EBook'),\n",
              " ('EBook', '2701')]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freq_dist = FreqDist(bigrams)\n",
        "rare_ngrams = [ngram for ngram, freq in freq_dist.items() if freq == 1]\n",
        "rare_ngrams[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5H5CnBawt5k"
      },
      "source": [
        "## Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wje1CtzfkBc6"
      },
      "source": [
        "### 1 Open The Adventures of Sherlock Holmes by Arthur Conan Doyle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luC7HZ6iwt5l"
      },
      "outputs": [],
      "source": [
        "with open(\"contents/Adventures_Holmes.txt\", 'r') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsdsp27ikBc7"
      },
      "source": [
        "### 2. Get one paragraph as test data, and the rest of paragraphs as train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud-y6nuGzrrL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from nltk import bigrams, word_tokenize\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "import random\n",
        "\n",
        "# Split the text into paragraphs\n",
        "paragraphs = text.split('\\n\\n')\n",
        "\n",
        "# Select a random paragraph for testing and use the rest for training\n",
        "random_index = random.randint(0, len(paragraphs) - 1)\n",
        "test_paragraph = paragraphs[random_index]\n",
        "training_paragraphs = paragraphs[:random_index] + paragraphs[random_index + 1:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRbgmjBYz0kt",
        "outputId": "fbf4ed01-cb0c-4dd2-ec83-baac51c44ce6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2610, 2609)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(paragraphs), len(training_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuWzLC6hkBc8"
      },
      "source": [
        "### 3.For each word in the test set, predict the following word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGpclqLS0Zqz"
      },
      "outputs": [],
      "source": [
        "# Prepare the training data\n",
        "training_text = \"\\n\\n\".join(training_paragraphs)\n",
        "training_tokens = word_tokenize(training_text.lower())\n",
        "training_bigrams = list(bigrams(training_tokens))\n",
        "\n",
        "# Build the bigram model\n",
        "model = ConditionalFreqDist(training_bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Lsv1ZV0j6k",
        "outputId": "3101e031-655c-4402-9606-1c7081968043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current word: '“', Predicted next word: 'i'\n",
            "Current word: 'very', Predicted next word: 'much'\n",
            "Current word: 'truly', Predicted next word: 'formidable'\n",
            "Current word: 'yours', Predicted next word: ','\n"
          ]
        }
      ],
      "source": [
        "# Function to predict the next word\n",
        "def predict_next_word(word):\n",
        "    word = word.lower()\n",
        "    if word in model:\n",
        "        return model[word].max()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Tokenize the test paragraph and predict the next word for each token\n",
        "test_tokens = word_tokenize(test_paragraph.lower())\n",
        "predictions = [(word, predict_next_word(word)) for word in test_tokens[:-1]]  # Exclude the last token\n",
        "\n",
        "# Print the predictions\n",
        "for current_word, next_word in predictions:\n",
        "    print(f\"Current word: '{current_word}', Predicted next word: '{next_word}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Qozjf9kBc9"
      },
      "source": [
        "### 4. Calculate the performance of the language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yov8kMDUkBc9",
        "outputId": "4976e936-a79d-4756-c775-46e5b1b300f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 25.00%\n"
          ]
        }
      ],
      "source": [
        "#To calculate the performance of the model, we can compare the predicted next words with the actual next words in the test paragraph.\n",
        "\n",
        "# Function to calculate the accuracy of the model\n",
        "def calculate_accuracy(predictions, test_tokens):\n",
        "    correct_predictions = sum(1 for i in range(len(predictions)) if predictions[i][1] == test_tokens[i + 1])\n",
        "    return correct_predictions / len(predictions)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = calculate_accuracy(predictions, test_tokens)\n",
        "print(f\"Model accuracy: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB8ZUSMYkBc9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}