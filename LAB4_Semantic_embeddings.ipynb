{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4 - Semantic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 – Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import spacy library and load the en_core_web_md model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spacy library\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the English model\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Show the word vector for “football”, how long is it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the representation of the word \"football\"\n",
    "football = nlp(\"football\")\n",
    "print(football.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Show the word vector for “frankfurteria”, how long is it?\n",
    "\n",
    "Only words in model’s vocabulary have vectors, the rest are called out-ofvocabulary (OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the representation of the word \"frankfurteria\" and show the length of the vector\n",
    "\n",
    "vec_frankfurteria = nlp(\"frankfurteria\").vector\n",
    "\n",
    "print(vec_frankfurteria, \"\\nLenght of the vector:\", len(vec_frankfurteria))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see frankfurteria is an Out Of Vocabulary word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. Check whether the word “flowers” is in the model vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the word \"flowers\" is in the vocabulary\n",
    "\n",
    "if \"flowers\" in nlp.vocab.strings:\n",
    "  print(\"The word 'flowers' is in the vocabulary\")\n",
    "\n",
    "else:\n",
    "  print(\"The word 'flowers' is not in the vocabulary\")\n",
    "\n",
    "#Get the representation of the word \"flowers\"\n",
    "#print(nlp(\"flowers\").vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a sentence including the word ”football”, and show the sentence vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Football is a great sport\"\n",
    "sent_vec = nlp(sentence).vector\n",
    "print(sent_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How long is the sentence vector? How is it calculated?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLenght of the vector:\", len(sent_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the sentence have many words, the length is 300 because they do the average of the vectors of all the sentence \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 – Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the two utterances “I visited Scotland” and “I went to Edinburgh”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt1 = \"I visited Scotland\"\n",
    "utt2 = \"I went to Edinburgh\"\n",
    "utt1_vec = nlp(utt1).vector\n",
    "utt2_vec = nlp(utt2).vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate the similarity between these two sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt1 = nlp(\"I visited Scotland\")\n",
    "utt2 = nlp(\"I went to Edinburgh\")\n",
    "\n",
    "utt1.similarity(utt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " \n",
    " \n",
    "# compute cosine similarity\n",
    "cosine = np.dot(utt1_vec,utt2_vec)/(norm(utt1_vec)*norm(utt2_vec))\n",
    "print(\"The cosine similarity is:\",cosine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two similar sentences and calculate their similarity,\n",
    "then define two very different sentences and calculate their similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarities of 2 similar sentences\n",
    "sent_1 = \"I do not like football\"\n",
    "sent_2 = \"I hate soccer\"\n",
    "sent_1_vec = nlp(sent_1).vector\n",
    "sent_2_vec = nlp(sent_2).vector\n",
    "cosine = np.dot(sent_1_vec,sent_2_vec)/(norm(sent_1_vec)*norm(sent_2_vec))\n",
    "print(f\"The cosine similarity for sentences: 'I do not like football' and 'I hate soccer' is:\\n{cosine}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarities of 2 dissimilar sentences\n",
    "sent_1 = \"I do not like football\"\n",
    "sent_2 = \"I love soccer\"\n",
    "sent_1_vec = nlp(sent_1).vector\n",
    "sent_2_vec = nlp(sent_2).vector\n",
    "cosine = np.dot(sent_1_vec,sent_2_vec)/(norm(sent_1_vec)*norm(sent_2_vec))\n",
    "print(f\"The cosine similarity for sentences: 'I do not like football' and 'I hate soccer' is:\\n{cosine}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Consider the following words [cat, dog, tiger, elephant, bird, monkey, lion, cheetah, burger, pizza, food, cheese, wine, salad, noodles, fruit, vegetables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_list = []\n",
    "word_list = [\"cat\", \"dog\", \"tiger\", \"elephant\", \"bird\", \"monkey\", \"lion\",\"cheetah\", \"burger\", \"pizza\", \"food\", \"cheese\", \"wine\", \"salad\", \"noodles\", \"fruit\", \"vegetables\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate the word vector for every word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_list:\n",
    "    embed_list.append(nlp(word).vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Apply a PCA, consider the first two components, and epresent the words in the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "pca = PCA(n_components=2)\n",
    "embed_pca = pca.fit_transform(embed_list)\n",
    "\n",
    "x = [vector[0] for vector in embed_pca] # x-axis\n",
    "y = [vector[1] for vector in embed_pca] # y-axis\n",
    "\n",
    "plt.scatter(x,y, marker='o') #\n",
    "for i, word in enumerate(word_list):\n",
    "    plt.annotate(word, (x[i], y[i]))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "#Print the amount of variance explained by each of the selected components\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "print(f\"The amount of variance explained by each of the selected components is:\\n component 1: {var_explained[0]}\\n component 2: {var_explained[1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the amount of variance explained by each of the selected components\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "print(f\"The amount of variance explained by each of the selected components is:\\n component 1: {var_explained[0]}\\n component 2: {var_explained[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do it in 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 3D PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "embed_pca = pca.fit_transform(embed_list)\n",
    "\n",
    "x = [vector[0] for vector in embed_pca] # x-axis\n",
    "y = [vector[1] for vector in embed_pca] # y-axis\n",
    "z = [vector[2] for vector in embed_pca] # z-axis\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, marker='o')\n",
    "\n",
    "for i, word in enumerate(word_list):\n",
    "    ax.text(x[i], y[i], z[i], word)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "#Print the amount of variance explained by each of the selected components\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "print(f\"The amount of variance explained by each of the selected components is:\\n component 1: {var_explained[0]}\\n component 2: {var_explained[1]}\\n component 3: {var_explained[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the amount of variance explained by each of the selected components\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "print(f\"The amount of variance explained by each of the selected components is:\\n component 1: {var_explained[0]}\\n component 2: {var_explained[1]}\\n component 3: {var_explained[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new set of words (at least 20 different words), and\n",
    "represent them in the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = ['Apple', 'Banana', 'Grapes', 'Pear', 'Orange', 'Melon', 'Tomato', 'pineaple', \n",
    "                'raspberry', 'watermelon', 'Information', 'Data', 'Bit', 'Computer', 'Mouse', \n",
    "                'Tower', 'Screen', 'Music', 'Network', 'Phone']\n",
    "embed_list1 =[]\n",
    "for word1 in common_words:\n",
    "    embed_list1.append(nlp(word1).vector)\n",
    "pca = PCA(n_components=2)\n",
    "embed_pca1 = pca.fit_transform(embed_list1)\n",
    "\n",
    "x = [vector1[0] for vector1 in embed_pca1] # x-axis\n",
    "y = [vector1[1] for vector1 in embed_pca1] # y-axis\n",
    "\n",
    "plt.scatter(x,y, marker='o') #\n",
    "for i, word1 in enumerate(common_words):\n",
    "    plt.annotate(word1, (x[i], y[i]))\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Print the amount of variance explained by each of the selected components\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "print(f\"The amount of variance explained by each of the selected components is:\\n component 1: {var_explained[0]}\\n component 2: {var_explained[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the amount of variance explained by each of the selected components\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "print(f\"The amount of variance explained by each of the selected components is:\\n component 1: {var_explained[0]}\\n component 2: {var_explained[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "common_words = ['Apple', 'Banana', 'Grapes', 'Pear', 'Orange', 'Melon', 'Tomato', 'pineaple', \n",
    "                'raspberry', 'watermelon', 'Information', 'Data', 'Bit', 'Computer', 'Mouse', \n",
    "                'Tower', 'Screen', 'Music', 'Network', 'Phone']\n",
    "# 3D PCA\n",
    "embed_list =[]\n",
    "for word in common_words:\n",
    "    embed_list.append(nlp(word).vector)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "embed_pca = pca.fit_transform(embed_list)\n",
    "\n",
    "x = [vector[0] for vector in embed_pca] # x-axis\n",
    "y = [vector[1] for vector in embed_pca] # y-axis\n",
    "z = [vector[2] for vector in embed_pca] # z-axis\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, marker='o')\n",
    "\n",
    "for i, word in enumerate(common_words):\n",
    "    ax.text(x[i], y[i], z[i], word)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "#Print the amount of variance explained by each of the selected components\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "print(f\"The amount of variance explained by each of the selected components is:\\n component 1: {var_explained[0]}\\n component 2: {var_explained[1]}\\n component 3: {var_explained[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_explained = pca.explained_variance_ratio_\n",
    "print(f\"The amount of variance explained by each of the selected components is:\\n component 1: {var_explained[0]}\\n component 2: {var_explained[1]}\\n component 3: {var_explained[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise IV – Categorizing text with semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define a set of sentences, e.g., “I purchased a science fiction book last week. I loved this fragrance: light, floral and feminine. I purchased a bottle of wine.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nlp(\"I purchased a science fiction book last week. I loved this fragrance: light, floral and feminine. I purchased a bottle of wine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a keyword, e.g., perfume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = nlp(\"perfume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate the similarity between each sentence and the keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity between the sentence and the keyword\n",
    "sent.similarity(keyword)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Could we filter out the sentences which are not related with the keyword?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter words that are not related with the sentence\n",
    "#filtered_sent = [word for word in sent if word.is_alpha and not word.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Alexa’s review dataset, and filter out the reviews\n",
    "which are not associated with the “music” property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40668\\3061529228.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'contents/amazon_alexa.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfiltered_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'verified_reviews'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"music\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\neild\\Miniconda3\\envs\\pytorch-env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('contents/amazon_alexa.tsv', delimiter = '\\t')\n",
    "\n",
    "filtered_df = data[data['verified_reviews'].lower().contains(\"music\")]\n",
    "filtered_df\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
