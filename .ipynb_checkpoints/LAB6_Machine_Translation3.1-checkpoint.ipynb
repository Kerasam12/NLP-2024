{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"SOS\", 2:\"<EOS>\"}\n",
    "        self.n_words = 3 \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs_train_train_train, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "[['hi', 'ciao !'], ['run !', 'corri !'], ['run !', 'corra !'], ['run !', 'correte !'], ['who ?', 'chi ?'], ['wow !', 'wow !'], ['jump !', 'salta !'], ['jump !', 'salti !'], ['jump !', 'saltate !'], ['jump', 'salta']]\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = readLangs('eng', 'ita', False)\n",
    "print(pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of the sentences in the dataset is 101 for the first language and 94 for the second language\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH_lang1 = max([len(pair[0].split(' ')) for pair in pairs])\n",
    "MAX_LENGTH_lang2 = max([len(pair[1].split(' ')) for pair in pairs])\n",
    "print(f\"The maximum length of the sentences in the dataset is {MAX_LENGTH_lang1} for the first language and {MAX_LENGTH_lang2} for the second language\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Max length would be set to the maximum length of any sentence in the dataset\n",
    "MAX_LENGTH = max(MAX_LENGTH_lang1, MAX_LENGTH_lang2) +1\n",
    "\n",
    "#below theres a trimming for shorter training time, but for full dataset this isnt used\n",
    "\n",
    "#If you want to trim also max length, uncomment the following line\n",
    "MAX_LENGTH = 8\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if len(pair[0].split(' ')) < MAX_LENGTH and len(pair[1].split(' ')) < MAX_LENGTH ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 345244 sentence pairs\n",
      "Trimmed to 253276 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "ita 22285\n",
      "eng 11042\n",
      "['ho mostrato il mio biglietto alla porta', 'i showed my ticket at the door']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    '''\n",
    "    -Read text file and split into lines, split lines into pairs\n",
    "    -Normalize text, filter by length \n",
    "    -Make word lists from sentences in pairs\n",
    "    '''\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    #Comment out the following line for full dataset\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'ita', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    '''\n",
    "    The EncoderRNN class is responsible for the encoder part of the seq2seq model.\n",
    "    ''' \n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #Embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        #GRU layer\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=False)\n",
    "        #Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Embedding layer with dropout\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #Get the output and hidden state from the GRU\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 8, 256])\n",
      "Output tensor: tensor([[[ 0.3049, -0.1757,  0.1206,  ...,  0.2703,  0.6308, -0.0399],\n",
      "         [-0.4789,  0.2560, -0.1512,  ...,  0.0021,  0.2494, -0.3869],\n",
      "         [-0.0905,  0.1449, -0.4662,  ...,  0.2685, -0.0764,  0.1012],\n",
      "         ...,\n",
      "         [-0.0696,  0.0573, -0.3923,  ...,  0.0088, -0.2134,  0.5970],\n",
      "         [-0.2709,  0.0632, -0.2513,  ...,  0.0079, -0.2100, -0.1275],\n",
      "         [-0.0375, -0.1229,  0.5289,  ...,  0.2515, -0.2660, -0.0690]]],\n",
      "       device='cuda:1', grad_fn=<CudnnRnnBackward0>)\n",
      "Hidden shape: torch.Size([1, 8, 256])\n",
      "Hidden tensor: tensor([[[ 0.3049, -0.1757,  0.1206,  ...,  0.2703,  0.6308, -0.0399],\n",
      "         [-0.4789,  0.2560, -0.1512,  ...,  0.0021,  0.2494, -0.3869],\n",
      "         [-0.0905,  0.1449, -0.4662,  ...,  0.2685, -0.0764,  0.1012],\n",
      "         ...,\n",
      "         [-0.0696,  0.0573, -0.3923,  ...,  0.0088, -0.2134,  0.5970],\n",
      "         [-0.2709,  0.0632, -0.2513,  ...,  0.0079, -0.2100, -0.1275],\n",
      "         [-0.0375, -0.1229,  0.5289,  ...,  0.2515, -0.2660, -0.0690]]],\n",
      "       device='cuda:1', grad_fn=<CudnnRnnBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Example of usage for EncoderRNN\n",
    "hidden_size = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "input_tensor = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]]).to(device)\n",
    "output, hidden = encoder(input_tensor)\n",
    "print(f\"Output shape: {output.shape}\") #Output shape is (1, 8, hidden_size) [BxSxH]\n",
    "print(f\"Output tensor: {output}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\") #Hidden shape is (1, 1, hidden_size) [BxSxH]\n",
    "print(f\"Hidden tensor: {hidden}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output shape: torch.Size([1, 8, 128])\n",
      "Encoder Hidden shape: torch.Size([1, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "# Example of usage for EncoderRNN\n",
    "output, hidden = encoder(input_tensor)\n",
    "print(f\"Encoder Output shape: {output.shape}\")\n",
    "print(f\"Encoder Hidden shape: {hidden.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Simple attention mechanism using dot product\n",
    "    '''\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        print(f\"Query shape before any operations: {query.shape}\")  # Debug\n",
    "        print(f\"Keys shape before any operations: {keys.shape}\")  # Debug\n",
    "\n",
    "        # Ensure the query is [batch_size, 1, hidden_size]\n",
    "        query = query.permute(1, 0, 2)  # Change to [batch_size, num_layers, hidden_size]\n",
    "        query = query.mean(dim=1, keepdim=True)  # Optional: Reduce across layers if necessary, resulting in [batch_size, 1, hidden_size]\n",
    "\n",
    "        # Transform query and keys\n",
    "        transformed_query = self.Wa(query)  # Shape: [batch_size, 1, hidden_size]\n",
    "        transformed_keys = self.Ua(keys)    # Shape: [batch_size, seq_len, hidden_size]\n",
    "        print(f\"Transformed Query shape: {transformed_query.shape}\")\n",
    "        print(f\"Transformed Keys shape: {transformed_keys.shape}\")\n",
    "        # This is where your existing prints are helpful too\n",
    "\n",
    "        # Since we're adding, shapes are:\n",
    "        # transformed_query: [batch_size, 1, hidden_size]\n",
    "        # transformed_keys: [batch_size, seq_len, hidden_size]\n",
    "        # Broadcasting takes care of matching dimensions\n",
    "\n",
    "        # Combined: Add transformed query and keys\n",
    "        combined = torch.tanh(transformed_query + transformed_keys)  # Broadcasting happens here\n",
    "\n",
    "        # Compute scores and weights\n",
    "        scores = self.Va(combined).squeeze(-1)  # Shape becomes [batch_size, seq_len]\n",
    "        weights = F.softmax(scores, dim=-1).unsqueeze(1)  # Shape: [batch_size, 1, seq_len]\n",
    "\n",
    "        # Compute context vector\n",
    "        context = torch.bmm(weights, keys)  # Shape: [batch_size, 1, hidden_size]\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape before any operations: torch.Size([1, 8, 256])\n",
      "Keys shape before any operations: torch.Size([1, 8, 256])\n",
      "Transformed Query shape: torch.Size([8, 1, 256])\n",
      "Transformed Keys shape: torch.Size([1, 8, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [8, 8] but got: [1, 8].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m, hidden_size)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m#shape: (1, 1, hidden_size) where hidden_size is 256 and 1 is the batch size and 1 is the sequence length\u001b[39;00m\n\u001b[1;32m      5\u001b[0m keys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m, hidden_size)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m#shape: (1, 8, hidden_size) where hidden_size is 256 and 1 is the batch size and 8 is the sequence length\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m context, weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#Context shape is (1, 1, hidden_size) [BxSxH]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext tensor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 40\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query, keys)\u001b[0m\n\u001b[1;32m     37\u001b[0m weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: [batch_size, 1, seq_len]\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Compute context vector\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, 1, hidden_size]\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context, weights\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [8, 8] but got: [1, 8]."
     ]
    }
   ],
   "source": [
    "#Check attention works\n",
    "hidden_size = 256\n",
    "attention = Attention(hidden_size).to(device)\n",
    "query = torch.randn(1, 8, hidden_size).to(device) #shape: (1, 1, hidden_size) where hidden_size is 256 and 1 is the batch size and 1 is the sequence length\n",
    "keys = torch.randn(1, 8, hidden_size).to(device) #shape: (1, 8, hidden_size) where hidden_size is 256 and 1 is the batch size and 8 is the sequence length\n",
    "context, weights = attention(query, keys)\n",
    "print(f\"Context shape: {context.shape}\") #Context shape is (1, 1, hidden_size) [BxSxH]\n",
    "print(f\"Context tensor: {context}\")\n",
    "print(f\"Weights shape: {weights.shape}\") #Weights shape is (1, 1, 8) [BxSxL]\n",
    "print(f\"Weights tensor: {weights}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The AttnDecoderRNN class is responsible for the decoder part of the seq2seq model.\n",
    "    This decoder has an attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            # Teacher forcing: Feed the target as the next input with some probability\n",
    "            if target_tensor is not None and random.random() < 0.5:\n",
    "                    decoder_input = target_tensor[:, i].unsqueeze(1)# Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attention_weights = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attention_weights\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "'''\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.tensor([[SOS_token] * batch_size], device=device).transpose(0, 1)\n",
    "        decoder_hidden = encoder_hidden[-1].unsqueeze(0)  # Taking the last layer's hidden state\n",
    "\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None and random.random() < 0.5:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # Next input is decoder's own current output\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        # Ensure `input` is correctly shaped for embedding\n",
    "        if input.dim() == 1:\n",
    "            input = input.unsqueeze(1)  # Reshape to [batch_size, 1] if necessary\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))  # [batch_size, 1, hidden_size]\n",
    "        \n",
    "        # Attention query preparation: Use the last hidden state for query\n",
    "        # Assuming `hidden` shape is [num_layers, batch_size, hidden_size] for GRUs/LSTMs\n",
    "        query = hidden[-1].unsqueeze(0)  # [1, batch_size, hidden_size], taking the last layer's hidden state\n",
    "        query = query.permute(1, 0, 2)  # Reshape to [batch_size, 1, hidden_size] for attention\n",
    "        \n",
    "        # Context calculation\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)  # encoder_outputs is [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Concatenation correction: Ensure both are [batch_size, 1, hidden_size]\n",
    "        # `context` should already be [batch_size, 1, hidden_size] if coming from your corrected attention forward\n",
    "        input_gru = torch.cat((embedded, context), dim=2)  # Correct concatenation\n",
    "        \n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output.squeeze(1))  # Assuming batch_first=True, squeeze seq_len dimension which is 1\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "    '''def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        query = hidden\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        \n",
    "        # Concatenate the embedded input and the context vector\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        output, hidden = self.gru(input_gru, query)\n",
    "        output = self.out(output.squeeze(dim=1))\n",
    "\n",
    "        return output, hidden, attn_weights'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape before any operations: torch.Size([2, 1, 256])\n",
      "Keys shape before any operations: torch.Size([2, 10, 256])\n",
      "Query shape before any operations: torch.Size([2, 1, 256])\n",
      "Keys shape before any operations: torch.Size([2, 10, 256])\n",
      "Query shape before any operations: torch.Size([2, 1, 256])\n",
      "Keys shape before any operations: torch.Size([2, 10, 256])\n",
      "Query shape before any operations: torch.Size([2, 1, 256])\n",
      "Keys shape before any operations: torch.Size([2, 10, 256])\n",
      "Query shape before any operations: torch.Size([2, 1, 256])\n",
      "Keys shape before any operations: torch.Size([2, 10, 256])\n",
      "Query shape before any operations: torch.Size([2, 1, 256])\n",
      "Keys shape before any operations: torch.Size([2, 10, 256])\n",
      "Query shape before any operations: torch.Size([2, 1, 256])\n",
      "Keys shape before any operations: torch.Size([2, 10, 256])\n",
      "Query shape before any operations: torch.Size([2, 1, 256])\n",
      "Keys shape before any operations: torch.Size([2, 10, 256])\n",
      "Output shape: torch.Size([2, 178280])\n",
      "Hidden shape: torch.Size([1, 2, 256])\n",
      "Attention shape: torch.Size([2, 8, 10])\n"
     ]
    }
   ],
   "source": [
    "# Mock input parameters\n",
    "batch_size = 2  # Example batch size\n",
    "seq_len = 10  # Example sequence length\n",
    "vocab_size = output_lang.n_words  # Assuming output_lang.n_words gives the target vocab size\n",
    "\n",
    "# Initialize the decoder\n",
    "decoder = AttnDecoderRNN(hidden_size=256, output_size=vocab_size, dropout_p=0.1).to(device)\n",
    "\n",
    "# Mock encoder outputs and hidden state\n",
    "mock_encoder_outputs = torch.rand(batch_size, seq_len, 256, device=device)  # [batch_size, seq_len, hidden_size]\n",
    "mock_encoder_hidden = torch.rand(1, batch_size, 256, device=device)  # [1, batch_size, hidden_size] for GRU/LSTM\n",
    "\n",
    "# Optionally, mock target tensor for teacher forcing\n",
    "# Here, assuming a random target sequence of length `seq_len`\n",
    "mock_target_tensor = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len), device=device)\n",
    "\n",
    "# Perform a forward pass through the decoder\n",
    "# Note: The `mock_target_tensor` is optional; it's used here to demonstrate teacher forcing\n",
    "output, hidden, attention = decoder(mock_encoder_outputs, mock_encoder_hidden, mock_target_tensor)\n",
    "\n",
    "# Display output shapes and a sample output\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: [batch_size, seq_len, vocab_size] after log_softmax\n",
    "print(f\"Hidden shape: {hidden.shape}\")  # Expected: [1, batch_size, hidden_size]\n",
    "print(f\"Attention shape: {attention.shape}\")  # Expected: [batch_size, seq_len, seq_len] or similar, depending on implementation\n",
    "\n",
    "# Note: This is a simplified test. Adjust `seq_len`, `batch_size`, and other parameters as needed to match your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    '''\n",
    "    Get the indexes of the words in the sentence by using the Lang class and its word2index dictionary\n",
    "    '''\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_dataloader(batch_size):\n",
    "    #Prepare the data for the training\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'ita', False)\n",
    "    pairs_train, pairs_test = train_test_split(pairs, test_size=0.1, random_state=42)\n",
    "    \n",
    "    n_train = len(pairs_train)\n",
    "    n_test = len(pairs_test)\n",
    "    \n",
    "    train_input_ids = np.zeros((n_train, MAX_LENGTH), dtype=np.int32)\n",
    "    train_target_ids = np.zeros((n_train, MAX_LENGTH), dtype=np.int32)\n",
    "    \n",
    "    test_input_ids = np.zeros((n_test, MAX_LENGTH), dtype=np.int32)\n",
    "    test_target_ids = np.zeros((n_test, MAX_LENGTH), dtype=np.int32)\n",
    "    \n",
    "    for idx, (inp, tgt) in enumerate(pairs_train):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        train_input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        train_target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "    \n",
    "    for idx, (inp, tgt) in enumerate(pairs_test):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        test_input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        test_target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(train_input_ids).to(device),\n",
    "                               torch.LongTensor(train_target_ids).to(device))\n",
    "\n",
    "    test_data = TensorDataset(torch.LongTensor(train_input_ids).to(device),\n",
    "                                torch.LongTensor(train_target_ids).to(device))\n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader, test_dataloader, pairs_train, pairs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 345244 sentence pairs\n",
      "Trimmed to 253276 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 11042\n",
      "ita 22285\n",
      "Train dataloader size: 7124\n",
      "Test dataloader size: 7124\n"
     ]
    }
   ],
   "source": [
    "#Example of usage for get_dataloader\n",
    "batch_size = 32\n",
    "input_lang, output_lang, train_dataloader, test_dataloader, pairs_train, pairs_test = get_dataloader(batch_size)\n",
    "print(f\"Train dataloader size: {len(train_dataloader)}\")\n",
    "print(f\"Test dataloader size: {len(test_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, test_dataloader, clip=1.0):\n",
    "    \n",
    "    #Set the models in training mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    #Initialize the total loss\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    # Iterate over the training dataloader\n",
    "    for input_tensor, target_tensor in train_dataloader:\n",
    "        \n",
    "        #Zero the gradients of the optimizers\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        #Get the input and target lengths\n",
    "        input_length = input_tensor.size(1)\n",
    "        target_length = target_tensor.size(1)\n",
    "        \n",
    "        #Initialize the encoder hidden state and the encoder outputs to zero-tensors\n",
    "        encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "        encoder_hidden = torch.zeros(1, 1, encoder.hidden_size, device=device)\n",
    "        \n",
    "        #Zero the loss\n",
    "        loss = 0\n",
    "\n",
    "        #Get the encoder outputs and hidden state\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor) \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device, dtype=torch.float) #SOS token as the first input to the decoder\n",
    "        decoder_hidden = encoder_hidden #Use the last hidden state of the encoder as the first hidden state of the decoder\n",
    "\n",
    "        #Iterate over the target length for the decoder to get the outputs\n",
    "        for di in range(target_length):\n",
    "            #Get the decoder output and hidden state and the attention weights\n",
    "            decoder_output, decoder_hidden, attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            #update the loss according to the criterion \n",
    "            loss += criterion(decoder_output, target_tensor[:, di])\n",
    "            #The next input to the decoder is the target tensor at the current position\n",
    "            decoder_input = target_tensor[:, di].unsqueeze(1)\n",
    "        \n",
    "        #Backward pass \n",
    "        loss.backward()\n",
    "        #Clip the gradients to avoid exploding gradients problem\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "        \n",
    "        #Update the parameters\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        #Add the loss to the total loss, as the loss is a tensor, we need to get the scalar value\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()  \n",
    "    # Iterate over the test dataloader\n",
    "    for input_tensor, target_tensor in test_dataloader:\n",
    "        \n",
    "        #Get the input and target lengths\n",
    "        input_length = input_tensor.size(1)\n",
    "        target_length = target_tensor.size(1)\n",
    "        \n",
    "        #Initialize the encoder hidden state and the encoder outputs to zero-tensors\n",
    "        encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "        encoder_hidden = torch.zeros(1, 1, encoder.hidden_size, device=device)\n",
    "        \n",
    "        #Zero the loss\n",
    "        loss = 0\n",
    "\n",
    "        #Get the encoder outputs and hidden state\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor) \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device) #SOS token as the first input to the decoder\n",
    "        decoder_hidden = encoder_hidden #Use the last hidden state of the encoder as the first hidden state of the decoder\n",
    "\n",
    "        #Iterate over the target length for the decoder to get the outputs\n",
    "        for di in range(target_length):\n",
    "            #Get the decoder output and hidden state and the attention weights\n",
    "            decoder_output, decoder_hidden, attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            #update the loss according to the criterion \n",
    "            loss += criterion(decoder_output, target_tensor[:, di])\n",
    "            #The next input to the decoder is the target tensor at the current position\n",
    "            decoder_input = target_tensor[:, di].unsqueeze(1)\n",
    "        \n",
    "        #Add the loss to the total loss, as the loss is a tensor, we need to get the scalar value\n",
    "        total_val_loss += loss.item()\n",
    "        \n",
    "    #Return the average loss\n",
    "    return total_train_loss / len(train_dataloader), total_val_loss / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.savefig('./loss_plots/loss_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, n_epochs, test_dataloader, clip=1.0):\n",
    "    plot_losses = []\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        train_loss, val_loss = train_epoch(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, test_dataloader, clip)\n",
    "        plot_losses.append((train_loss, val_loss))\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} Train loss: {train_loss} Val loss: {val_loss}\")\n",
    "        #Save the model after each epoch\n",
    "        torch.save(encoder.state_dict(), f'./models/encoder_epoch_{epoch+1}.pth')\n",
    "        torch.save(decoder.state_dict(), f'./models/decoder_epoch_{epoch+1}.pth')\n",
    "    showPlot(plot_losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def showAttention(input_sentence, output_words, attentions, plot_path):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence, plot_path,encoder, decoder):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :], plot_path=plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 345244 sentence pairs\n",
      "Trimmed to 253276 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 11042\n",
      "ita 22285\n",
      "['i like your avatar', 'mi piace il vostro avatar']\n",
      "['they hugged', 'si sono abbracciati']\n"
     ]
    }
   ],
   "source": [
    "#we need to get the test pairs in the same format as the train pairs\n",
    "batch_size = 32\n",
    "_, _, _, _, pairs_train, pairs_test = get_dataloader(batch_size)\n",
    "print (pairs_test[0])\n",
    "print (pairs_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluateRandomly(encoder, decoder, n=20):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_test) #pairs is the list of \n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "        evaluateAndShowAttention(pair[0], f'./test_plots/plot_{i}.png',encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 345244 sentence pairs\n",
      "Trimmed to 253276 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 11042\n",
      "ita 22285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape before any operations: torch.Size([8, 1, 128])\n",
      "Keys shape before any operations: torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 128x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m decoder_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(decoder\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss(ignore_index\u001b[38;5;241m=\u001b[39mPAD_token)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Save the model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(encoder\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_short.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, n_epochs, test_dataloader, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m plot_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):\n\u001b[0;32m----> 7\u001b[0m     train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     plot_losses\u001b[38;5;241m.\u001b[39mappend((train_loss, val_loss))\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[56], line 37\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, test_dataloader, clip)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#Iterate over the target length for the decoder to get the outputs\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m di \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(target_length):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#Get the decoder output and hidden state and the attention weights\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     decoder_output, decoder_hidden, attention \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#update the loss according to the criterion \u001b[39;00m\n\u001b[1;32m     39\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(decoder_output, target_tensor[:, di])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 77\u001b[0m, in \u001b[0;36mAttnDecoderRNN.forward\u001b[0;34m(self, encoder_outputs, encoder_hidden, target_tensor)\u001b[0m\n\u001b[1;32m     74\u001b[0m attentions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_LENGTH):\n\u001b[0;32m---> 77\u001b[0m     decoder_output, decoder_hidden, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     decoder_outputs\u001b[38;5;241m.\u001b[39mappend(decoder_output)\n\u001b[1;32m     81\u001b[0m     attentions\u001b[38;5;241m.\u001b[39mappend(attn_weights)\n",
      "Cell \u001b[0;32mIn[52], line 108\u001b[0m, in \u001b[0;36mAttnDecoderRNN.forward_step\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    105\u001b[0m query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Reshape to [batch_size, 1, hidden_size] for attention\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Context calculation\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m context, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# encoder_outputs is [batch_size, seq_len, hidden_size]\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Concatenation correction: Ensure both are [batch_size, 1, hidden_size]\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# `context` should already be [batch_size, 1, hidden_size] if coming from your corrected attention forward\u001b[39;00m\n\u001b[1;32m    112\u001b[0m input_gru \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embedded, context), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Correct concatenation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 22\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query, keys)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Transform query and keys\u001b[39;00m\n\u001b[1;32m     21\u001b[0m transformed_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWa(query)  \u001b[38;5;66;03m# Shape: [batch_size, 1, hidden_size]\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m transformed_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# Shape: [batch_size, seq_len, hidden_size]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Since we're adding, shapes are:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# transformed_query: [batch_size, 1, hidden_size]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# transformed_keys: [batch_size, seq_len, hidden_size]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Broadcasting takes care of matching dimensions\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Combined: Add transformed query and keys\u001b[39;00m\n\u001b[1;32m     30\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(transformed_query \u001b[38;5;241m+\u001b[39m transformed_keys)  \u001b[38;5;66;03m# Broadcasting happens here\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 128x128)"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_size = 128\n",
    "batch_size = 64\n",
    "n_epochs = 1\n",
    "input_lang, output_lang, train_dataloader, test_dataloader, pairs_train, pairs_test = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_token)\n",
    "\n",
    "train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, n_epochs, test_dataloader, clip=1.0)\n",
    "#Save the model\n",
    "torch.save(encoder.state_dict(), 'encoder_short.pth')\n",
    "torch.save(decoder.state_dict(), 'decoder_short.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 345244 sentence pairs\n",
      "Trimmed to 345244 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "ita 26170\n",
      "eng 13069\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#encoder.load_state_dict(torch.load('encoder.pth'))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#decoder.load_state_dict(torch.load('decoder.pth'))\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, encoder, decoder, n_epochs, learning_rate, print_every, plot_every)\u001b[0m\n\u001b[1;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     15\u001b[0m     plot_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     12\u001b[0m decoder_outputs, _, _ \u001b[38;5;241m=\u001b[39m decoder(encoder_outputs, encoder_hidden, target_tensor)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     15\u001b[0m     decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, decoder_outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     16\u001b[0m     target_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "#To continue training, load the model and continue training\n",
    "\n",
    "hidden_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "input_lang, output_lang, train_dataloader, test_dataloader, pairs_train, pairs_test = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "#encoder.load_state_dict(torch.load('encoder.pth'))\n",
    "#decoder.load_state_dict(torch.load('decoder.pth'))\n",
    "\n",
    "train(train_dataloader, encoder, decoder, n_epochs=1, print_every=5, plot_every=5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "#encoder = encoder.load_state_dict(torch.load('encoder_short.pth'))\n",
    "#encoder = decoder.load_state_dict(torch.load('decoder_short.pth'))\n",
    "\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
