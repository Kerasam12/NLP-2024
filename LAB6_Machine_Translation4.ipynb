{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 6: Machine Translation. Italian to English with Seq2Seq RNNs with attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives:\n",
    "1. Understand the seq2seq model architecture.\n",
    "2. Understand the attention mechanism.\n",
    "3. Implement a seq2seq model with attention mechanism for machine translation.\n",
    "4. Train the model on a Italian to English translation dataset.\n",
    "5. Translate Italian sentences to English.\n",
    "6. Evaluate the model using quantitative and qualitative evaluation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and SetUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Wandb for Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vc23ed8d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c7c908dffb45d2937924c0a502becc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/nlp_2024/Machine_Translation/runs/vc23ed8d' target=\"_blank\">https://wandb.ai/nlp_2024/Machine_Translation/runs/vc23ed8d</a><br/> View project at: <a href='https://wandb.ai/nlp_2024/Machine_Translation' target=\"_blank\">https://wandb.ai/nlp_2024/Machine_Translation</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240413_154531-vc23ed8d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vc23ed8d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ca202a0b5a4519abcd2ea30d3b0d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112506263371971, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/export/hhome/nlp2_g09/NLP-2024/wandb/run-20240413_154809-n1xtzyrs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_2024/Machine_Translation/runs/n1xtzyrs' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/nlp_2024/Machine_Translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_2024/Machine_Translation' target=\"_blank\">https://wandb.ai/nlp_2024/Machine_Translation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_2024/Machine_Translation/runs/n1xtzyrs' target=\"_blank\">https://wandb.ai/nlp_2024/Machine_Translation/runs/n1xtzyrs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp_2024/Machine_Translation/runs/n1xtzyrs?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5633496710>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"Machine_Translation\", entity=\"nlp_2024\", name=\"trial_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "Data can be found in our [github](https://github.com/Neilus03/NLP-2024/blob/main/data/eng-ita.txt).\n",
    "\n",
    "The dataset is a txt file that looks like this:\n",
    "\n",
    "```\n",
    "I haven't eaten pizza recently. Io non ho mangiato della pizza di recente.\n",
    "I haven't figured that out yet.\tNon l'ho ancora capito.\n",
    "Hello! Ciao!\n",
    "I saw you in the park with Tom.\tL'ho vista al parco con Tom.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Lang Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the start-of-sequence (SOS) and end-of-sequence (EOS) tokens\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# Define the language class\n",
    "class Lang:\n",
    "    '''\n",
    "    Lang class to store the language vocabulary and word-to-index & index-to-word mappings.\n",
    "    It also stores the count of each word in the vocabulary\n",
    "    '''\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "        self.word2index = {}  # dictionary to map words to indices\n",
    "        self.word2count = {}  # dictionary to count the occurrences of each word\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}  # dictionary to map indices to words\n",
    "        self.n_words = 2  # Count SOS and EOS tokens (initially 2)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        # Add each word in the sentence to the language\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        # Add a word to the language\n",
    "        if word not in self.word2index:\n",
    "            # If the word is not already in the dictionary, add it\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            # If the word is already in the dictionary, increment its count\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the Data a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(unicode_string):\n",
    "    # Convert unicode characters to ASCII\n",
    "    ascii_string = ''\n",
    "    for c in unicodedata.normalize('NFD', unicode_string): # NFD = Normalization Form Canonical Decomposition\n",
    "        if unicodedata.category(c) != 'Mn': # Mn = Nonspacing_Mark\n",
    "            ascii_string += c\n",
    "    return ascii_string\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    # Normalize the string by:\n",
    "    \n",
    "    # converting to lowercase,\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    #removing accents \n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #replacing non-alphabetic characters with spaces\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    #removing extra spaces\n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create read_data function, which reads the data from the file and returns the input and target language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(lang1, lang2, reverse=True, verbose=False):\n",
    "    '''\n",
    "    Read the data file, split the file into lines and split\n",
    "    lines into pairs. the `reverse` flag is used to translate from italian\n",
    "    to english instaed of the default english to italian\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Opening Data\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Create Lang instances\n",
    "    input_lang = Lang(lang1) if not reverse else Lang(lang2)\n",
    "    output_lang = Lang(lang2) if not reverse else Lang(lang1)\n",
    "    \n",
    "    # Leave pairs as they are if not reverse, otherwise reverse the pairs\n",
    "    pairs = [list(reversed(p)) for p in pairs] if reverse else pairs\n",
    "\n",
    "    # Return the language vocabulary and the pairs\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage of the read_data function, and how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['io sono entusiasta', 'i m enthusiastic'] ita eng\n",
      "['they re now alone', 'loro sono da soli adesso'] eng ita\n"
     ]
    }
   ],
   "source": [
    "#Example usage of the read_data \n",
    "input_lang, output_lang, pairs = read_data('eng', 'ita', reverse=True, verbose=False)\n",
    "print(random.choice(pairs), input_lang.language, output_lang.language)\n",
    "input_lang, output_lang, pairs = read_data('eng', 'ita', False, verbose=False)\n",
    "print(random.choice(pairs), input_lang.language, output_lang.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim the dataset to contain only 10000 examples and only phrases with less than 10 words.\n",
    "This is done to reduce the training time and to make the model learn faster.\n",
    "If you have a better GPU, you can increase the number of examples and the length of the phrases,\n",
    "sadly we are GPU-Poor hahaha, maybe Dani and Joan can grab some from the CVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "MAX_PAIRS = 100000\n",
    "\n",
    "def filterPair(p):\n",
    "    return (len(p[0].split(' ')) and len(p[1].split(' '))) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def trim_dataset(pairs):\n",
    "    # Filter pairs using the filterPair condition and choosing 100000 random pairs\n",
    "    pairs = [pair for pair in pairs if filterPair(pair)]\n",
    "    return random.sample(pairs, MAX_PAIRS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-   Read text file and split into lines, split lines into pairs\n",
    "-   Normalize text, filter by length and content\n",
    "-   Make word lists from sentences in pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Data\n",
      "Read 345244 sentence pairs\n",
      "Trimmed to 100000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 9236\n",
      "ita 17105\n",
      "['i brush my teeth twice a day', 'io mi lavo i denti due volte al giorno']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    ''''\n",
    "    Prepare the data for training by reading the data, filtering the pairs and counting the words\n",
    "    Returns the input and output language instances and the processed pairs\n",
    "    '''\n",
    "    # Read the data\n",
    "    input_lang, output_lang, pairs = read_data(lang1, lang2, reverse, verbose=True)\n",
    "    print(\"Read all the %s sentence pairs in the dataset\" % len(pairs))\n",
    "    # Filter the pairs\n",
    "    pairs = trim_dataset(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs using the trim_dataset function\" % len(pairs))\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(f\"There are {input_lang.n_words} words in {input_lang.language} and {output_lang.n_words} words in {output_lang.language}\")\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('eng', 'ita')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Test and Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training pairs is 70000\n",
      "         The number of validation pairs is 15000\n",
      "         The number of test pairs is 15000\n",
      "Training Pair: ['i was very busy yesterday', 'io ero molto impegnato ieri']\n",
      "Validation Pair: ['please give me a piece of bread', 'per favore dammi un pezzo di pane']\n",
      "Test Pair: ['you re unambitious', 'lei e priva di ambizioni']\n"
     ]
    }
   ],
   "source": [
    "# Divide data into training, validation and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pairs, test_val_pairs = train_test_split(pairs, test_size=0.3, random_state=42) #70% train, 30% test & val\n",
    "test_pairs, val_pairs = train_test_split(test_val_pairs, test_size=0.5, random_state=42) #15% test, 15% val\n",
    "\n",
    "#Check the amount of data in each set\n",
    "print(f\"The number of training pairs is {len(train_pairs)}\\n \\\n",
    "        The number of validation pairs is {len(val_pairs)}\\n \\\n",
    "        The number of test pairs is {len(test_pairs)}\")\n",
    "\n",
    "#Sample an example from the training set, validation set and test set\n",
    "print(f\"Training Pair: {random.choice(train_pairs)}\")\n",
    "print(f\"Validation Pair: {random.choice(val_pairs)}\")\n",
    "print(f\"Test Pair: {random.choice(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model\n",
    "\n",
    "The model is composed of an encoder and a decoder. The encoder reads the input sequence and outputs a context vector for each word in the input sequence. The decoder reads the context vector and generates the output sequence, hopefully translating the input sequence to the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.guru99.com/images/1/111318_0848_seq2seqSequ1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder of a seq2seq network is a RNN, in our case we'll use a GRU for the sake of simplicity.\n",
    "This GRU encoder outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    '''\n",
    "    EncoderRNN class to encode the input language, it uses an embedding layer and a GRU layer\n",
    "    basically it encodes the input language into a hidden state.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #Embedding layer to convert words to vectors of fixed size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        #GRU layer to encode the input language\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        #Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''Forward pass of the encoder'''\n",
    "        #Convert the input to embeddings and apply dropout\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #Pass the embeddings through the GRU layer and get the output and hidden state\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage of the EncoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden size\n",
    "hidden_size = 256\n",
    "\n",
    "#Create an instance of the EncoderRNN \n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "\n",
    "#input tensor, it has a batch size of 1 and a sequence length of 10\n",
    "input_tensor = torch.randint(0, input_lang.n_words, (1, 10)).to(device) #random input tensor [1, 10] with values between 0 and input_lang.n_words\n",
    "\n",
    "#output and hidden state of the encoder after passing the input tensor\n",
    "output, hidden = encoder(input_tensor)\n",
    "\n",
    "#Print interesting information\n",
    "print(f\"Input Tensor Shape: {input_tensor.shape}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Encoder Output: {output}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Hidden Shape: {hidden.shape}\")\n",
    "print(f\"Encoder Hidden: {hidden}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation, in this case we will use an Attention Decoder. In sequence-to-sequence models, an attention decoder lets the decoder focus on different parts of the encoder's output using a calculated set of \"attention weights\". These weights help create a weighted combination of encoder outputs (attn_applied), enhancing the decoder's ability to select accurate output words based on the input sequence's relevant parts. Here we use Bahdanau attention mechanism. The process involves training a feed-forward layer to calculate these weights, adjusted for varying sentence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    '''\n",
    "    The typical Badhanau Attention mechanism.\n",
    "        1. Calculate the attention scores by applying a linear layer to the encoder outputs and decoder hidden state\n",
    "        2. Apply a softmax to the scores to get the attention weights\n",
    "        3. Multiply the attention weights by the encoder outputs to get the context vector\n",
    "        4. Return the context vector and the attention weights\n",
    "    '''\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        #Linear layers to calculate the attention scores\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size) #W matrix [hidden_size x hidden_size]\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size) #U matrix [hidden_size x hidden_size]\n",
    "        self.Va = nn.Linear(hidden_size, 1) #V vector [hidden_size x 1]\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        #Get the sum of the projection of the query and keys\n",
    "        addition = self.Wa(query) + self.Ua(keys) #Addition shape is [batch_size x seq_len x hidden_size]\n",
    "        \n",
    "        #Apply a non-linearity to the sum (tanh in this case because we want to keep the values between -1 and 1)\n",
    "        activated_addition = torch.tanh(addition)#Activated addition shape is [batch_size x seq_len x hidden_size]\n",
    "        \n",
    "        #Get the attention scores by applying the V matrix to the activated addition\n",
    "        scores = self.Va(activated_addition) #Scores shape right now is [batch_size x seq_len x 1]\n",
    "        \n",
    "        #Squeeze the scores to remove the last dimension and unsqueeze the scores to add a dimension at index 1\n",
    "        scores = scores.squeeze(2).unsqueeze(1) #Scores shape is now [batch_size x 1 x seq_len]\n",
    "\n",
    "        #Apply a softmax to the scores to get the attention weights adding up to 1\n",
    "        weights = F.softmax(scores, dim=-1) #Weights shape is [batch_size x 1 x seq_len]\n",
    "        \n",
    "        #Multiply the weights by the keys to get the context vector (bmm stands for batch matrix multiplication)\n",
    "        context = torch.bmm(weights, keys) #Context shape is [batch_size x 1 x hidden_size]\n",
    "\n",
    "        #Return the context vector and the attention weights.\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    '''\n",
    "    Attention Decoder class to decode the encoder outputs and the hidden \n",
    "    state into the target language, hopefully making a good translation.\n",
    "    It uses an embedding layer, an attention layer and a GRU layer \n",
    "    (you could add more layers if you want to improve the model)\n",
    "    '''\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True) #2 * hidden_size because we concatenate the embeddings and context vector\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None): #encoder outputs and encoder hidden are returned by the encoder\n",
    "        '''\n",
    "        Forward takes care of the forward pass of the decoder, it loops over the whole sequence length,\n",
    "        translating one word at a time by using the step_forward function.\n",
    "        '''\n",
    "        #Get the batch size\n",
    "        batch_size = encoder_outputs.size(0) \n",
    "        \n",
    "        #Initialize the decoder input with the SOS token for each sentence in the batch\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        \n",
    "        #Initialize the decoder hidden state with the encoder hidden state \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        #Initialize the decoder outputs and attention maps as empty lists\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            #Make a forward step in the decoder, get the output, hidden state and attention weights for the decoder\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            #Append the decoder output and attention weights to their respective lists\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            #If there is a target tensor, use it as the next input, otherwise use the decoder output\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1) #Get the index of the maximum value\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input, gradient is not computed for the input.\n",
    "\n",
    "        #Concatenate the decoder outputs and attention weights along the sequence length dimension\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1) #Apply a log softmax to the decoder outputs\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        #Return the decoder outputs and attention weights\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        '''\n",
    "        forward_step takes care of the forward pass of the decoder for a single time step (i.e. a single word)\n",
    "        '''\n",
    "        #Get the embeddings of the input with the applied dropout\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        #Permute the hidden state to have the batch size as the first dimension\n",
    "        query = hidden.permute(1, 0, 2) #from [1, batch_size, hidden_size] to [batch_size, 1, hidden_size]\n",
    "        \n",
    "        #Get the context vector and attention weights by applying the attention mechanism (Bahdanau)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        \n",
    "        #Concatenate the embeddings and context vector along the hidden size dimension\n",
    "        input_gru = torch.cat((embedded, context), dim=2) #Concatenation shape is [batch_size x 1 x 2 * hidden_size]\n",
    "        \n",
    "        #Pass the concatenated tensor through the GRU layer, get the output and hidden state\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        \n",
    "        #Pass the output through a linear layer to get the decoder output\n",
    "        output = self.out(output) #Now shape is [batch_size x 1 x output_size] \n",
    "                                  #where output_size is the number of words in the output language (i.e. Vocabulary size)\n",
    "\n",
    "        #Return the decoder output, hidden state and attention weights\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage of the AttnDecoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape: torch.Size([1, 10, 17105])\n",
      "Decoder Hidden Shape: torch.Size([1, 1, 256])\n",
      "Attentions Shape: torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "#Example usage of the AttnDecoderRNN\n",
    "\n",
    "#Create an instance of the AttnDecoderRNN\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "#Pass the encoder\n",
    "decoder_outputs, decoder_hidden, attentions = decoder(output, hidden)\n",
    "\n",
    "#Print interesting information\n",
    "print(f\"Decoder Outputs Shape: {decoder_outputs.shape}\")\n",
    "print(f\"Decoder Hidden Shape: {decoder_hidden.shape}\")\n",
    "print(f\"Attentions Shape: {attentions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preparing Training Data\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepare_data('eng', 'ita', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "==================\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the `<SOS>` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\\\"Teacher forcing\\\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder\\'s guess as the next\n",
    "input. Using teacher forcing causes it to converge faster but [when the\n",
    "trained network is exploited, it may exhibit\n",
    "instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation\n",
    "-intuitively it has learned to represent the output grammar and can\n",
    "\\\"pick up\\\" the meaning once the teacher tells it the first few words,\n",
    "but it has not properly learned how to create the sentence from the\n",
    "translation in the first place.\n",
    "\n",
    "Because of the freedom PyTorch\\'s autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "`teacher_forcing_ratio` up to use more of it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-   Start a timer\n",
    "-   Initialize optimizers and criterion\n",
    "-   Create set of training pairs\n",
    "-   Start empty losses array for plotting\n",
    "\n",
    "Then we call `train` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "================\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "`plot_losses` saved while training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder\\'s predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder\\'s\n",
    "attention outputs for display later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we\\'ll get\n",
    "some reasonable results.\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>If you run this notebook you can train, interrupt the kernel,evaluate, and continue training later. Comment out the lines where theencoder and decoder are initialized and run <code>trainIters</code> again.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 345244 sentence pairs\n",
      "Trimmed to 32643 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "ita 5466\n",
      "eng 3219\n",
      "2m 51s (- 42m 45s) (5 6%) 0.8493\n",
      "5m 48s (- 40m 41s) (10 12%) 0.1683\n",
      "8m 46s (- 38m 0s) (15 18%) 0.0803\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set dropout layers to `eval` mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "=====================\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run `plt.matshow(attentions)` to see attention output\n",
    "displayed as a matrix. For a better viewing experience we will do the\n",
    "extra work of adding axes and labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
    "\n",
    "\n",
    "evaluateAndShowAttention('io sono troppo stanco per guidare')\n",
    "\n",
    "evaluateAndShowAttention('tu sei un bravo ragazzo')\n",
    "\n",
    "evaluateAndShowAttention('ci sono facendo gli spaghetti')\n",
    "\n",
    "evaluateAndShowAttention('lui e molto carino')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-   Try with a different dataset\n",
    "    -   Another language pair\n",
    "    -   Human â†’ Machine (e.g. IOT commands)\n",
    "    -   Chat â†’ Response\n",
    "    -   Question â†’ Answer\n",
    "-   Replace the embeddings with pretrained word embeddings such as\n",
    "    `word2vec` or `GloVe`\n",
    "-   Try with more layers, more hidden units, and more sentences. Compare\n",
    "    the training time and results.\n",
    "-   If you use a translation file where pairs have two of the same\n",
    "    phrase (`I am test \\t I am test`), you can use this as an\n",
    "    autoencoder. Try this:\n",
    "    -   Train as an autoencoder\n",
    "    -   Save only the Encoder network\n",
    "    -   Train a new Decoder for translation from there\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
