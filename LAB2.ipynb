{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noSoe2Bfwt5U"
      },
      "source": [
        "# Exercise 1: N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJHSuILGwt5W",
        "outputId": "074ff6b0-a86a-4121-c0c8-e95b5d008bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb8YSY42wt5X"
      },
      "source": [
        "### 1. Consider the sentence “To Sherlock Holmes she is always ‘The Woman.’ I have seldom heard him mention her under any other name.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sy5h3oDvwt5Y"
      },
      "outputs": [],
      "source": [
        "sentence = \"To Sherlock Holmes she is always 'The Woman' I have seldom heard him mention her under any other name.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5luVvWZPwt5Z"
      },
      "source": [
        "### 2. Tokenize the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFGX9Showt5Z",
        "outputId": "0fc0b100-5ae1-45b9-8d20-b07715aba7ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['To',\n",
              " 'Sherlock',\n",
              " 'Holmes',\n",
              " 'she',\n",
              " 'is',\n",
              " 'always',\n",
              " \"'The\",\n",
              " 'Woman',\n",
              " \"'\",\n",
              " 'I',\n",
              " 'have',\n",
              " 'seldom',\n",
              " 'heard',\n",
              " 'him',\n",
              " 'mention',\n",
              " 'her',\n",
              " 'under',\n",
              " 'any',\n",
              " 'other',\n",
              " 'name',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tokens = nltk.word_tokenize(sentence)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH5rZiMkwt5a"
      },
      "source": [
        "### 3. Calculate the n-grams of the sentence, with n = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiIGLCurwt5b",
        "outputId": "e3a69c51-7552-48b1-d184-a2d5ee197db2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('To', 'Sherlock'),\n",
              " ('Sherlock', 'Holmes'),\n",
              " ('Holmes', 'she'),\n",
              " ('she', 'is'),\n",
              " ('is', 'always'),\n",
              " ('always', \"'The\"),\n",
              " (\"'The\", 'Woman'),\n",
              " ('Woman', \"'\"),\n",
              " (\"'\", 'I'),\n",
              " ('I', 'have'),\n",
              " ('have', 'seldom'),\n",
              " ('seldom', 'heard'),\n",
              " ('heard', 'him'),\n",
              " ('him', 'mention'),\n",
              " ('mention', 'her'),\n",
              " ('her', 'under'),\n",
              " ('under', 'any'),\n",
              " ('any', 'other'),\n",
              " ('other', 'name'),\n",
              " ('name', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "bigrams = list(nltk.bigrams(tokens))\n",
        "bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuh41_ZTwt5c",
        "outputId": "73029f1b-964c-4c21-f5e0-dbfc682a6dc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('To', 'Sherlock', 'Holmes'),\n",
              " ('Sherlock', 'Holmes', 'she'),\n",
              " ('Holmes', 'she', 'is'),\n",
              " ('she', 'is', 'always'),\n",
              " ('is', 'always', \"'The\"),\n",
              " ('always', \"'The\", 'Woman'),\n",
              " (\"'The\", 'Woman', \"'\"),\n",
              " ('Woman', \"'\", 'I'),\n",
              " (\"'\", 'I', 'have'),\n",
              " ('I', 'have', 'seldom'),\n",
              " ('have', 'seldom', 'heard'),\n",
              " ('seldom', 'heard', 'him'),\n",
              " ('heard', 'him', 'mention'),\n",
              " ('him', 'mention', 'her'),\n",
              " ('mention', 'her', 'under'),\n",
              " ('her', 'under', 'any'),\n",
              " ('under', 'any', 'other'),\n",
              " ('any', 'other', 'name'),\n",
              " ('other', 'name', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "trigrams = list(nltk.trigrams(tokens))\n",
        "trigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6fzzeGFwt5e"
      },
      "source": [
        "### 4. Perform text pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TSxZsC9wt5f"
      },
      "source": [
        "1. Remove special characters such as *,<,>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gr4ubbMwt5f",
        "outputId": "9ad2f7b8-d48d-469c-ef78-3380a34440a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', \"'The\", 'Woman', \"'\", 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.']\n"
          ]
        }
      ],
      "source": [
        "#Remove all the tokens that are equal to \"*\", \">\" or \"<\"\n",
        "prep_tok = [word for word in tokens if word not in [\"*\", \">\", \"<\"]]\n",
        "print(prep_tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eNKdp2Hwt5g"
      },
      "source": [
        "2. Remove punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26A7ZyVNwt5h",
        "outputId": "86cf9f2d-e7f1-43e9-8027-7cc4c82b1956"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['To',\n",
              " 'Sherlock',\n",
              " 'Holmes',\n",
              " 'she',\n",
              " 'is',\n",
              " 'always',\n",
              " \"'The\",\n",
              " 'Woman',\n",
              " 'I',\n",
              " 'have',\n",
              " 'seldom',\n",
              " 'heard',\n",
              " 'him',\n",
              " 'mention',\n",
              " 'her',\n",
              " 'under',\n",
              " 'any',\n",
              " 'other',\n",
              " 'name']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#Remove punctuation as well\n",
        "import string\n",
        "preprocessed_tokens = [word for word in prep_tok if word not in string.punctuation]\n",
        "preprocessed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_CWzAD1wt5h"
      },
      "source": [
        "### 5. Compute the frequency of each n-gram, and show the top 10 bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X4o_3-Awt5h",
        "outputId": "5cb6a5f9-eff7-4caa-ad15-96d13da466e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('To', 'Sherlock'), 1), (('Sherlock', 'Holmes'), 1), (('Holmes', 'she'), 1), (('she', 'is'), 1), (('is', 'always'), 1), (('always', \"'The\"), 1), ((\"'The\", 'Woman'), 1), (('Woman', 'I'), 1), (('I', 'have'), 1), (('have', 'seldom'), 1)]\n",
            "[(('To', 'Sherlock', 'Holmes'), 1), (('Sherlock', 'Holmes', 'she'), 1), (('Holmes', 'she', 'is'), 1), (('she', 'is', 'always'), 1), (('is', 'always', \"'The\"), 1), (('always', \"'The\", 'Woman'), 1), ((\"'The\", 'Woman', 'I'), 1), (('Woman', 'I', 'have'), 1), (('I', 'have', 'seldom'), 1), (('have', 'seldom', 'heard'), 1)]\n",
            "[(('To', 'Sherlock', 'Holmes', 'she'), 1), (('Sherlock', 'Holmes', 'she', 'is'), 1), (('Holmes', 'she', 'is', 'always'), 1), (('she', 'is', 'always', \"'The\"), 1), (('is', 'always', \"'The\", 'Woman'), 1), (('always', \"'The\", 'Woman', 'I'), 1), ((\"'The\", 'Woman', 'I', 'have'), 1), (('Woman', 'I', 'have', 'seldom'), 1), (('I', 'have', 'seldom', 'heard'), 1), (('have', 'seldom', 'heard', 'him'), 1)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "bigram_counts = Counter(list(nltk.bigrams(preprocessed_tokens)))\n",
        "print(bigram_counts.most_common(10))\n",
        "trigram_counts = Counter(list(nltk.trigrams(preprocessed_tokens)))\n",
        "print(trigram_counts.most_common(10))\n",
        "fourgram_counts = Counter(nltk.ngrams(preprocessed_tokens, 4))\n",
        "print(fourgram_counts.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXLIkIC_wt5i"
      },
      "source": [
        "Now Let's do the same with the **Moby Dick** text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s35__QS0wt5i"
      },
      "outputs": [],
      "source": [
        "with open(\"./moby_dick.txt\", 'r') as file:\n",
        "    mobydick = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDg9rAstwt5j",
        "outputId": "f9fd9560-e684-44dc-8ce5-fa83a0063ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in Moby Dick: 259422\n"
          ]
        }
      ],
      "source": [
        "mobydick_tokens = nltk.word_tokenize(mobydick)\n",
        "print(\"Number of tokens in Moby Dick:\", len(mobydick_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iTA3WBkpwt5j"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "#Now remove punctuations and \"*\", \"<\", \">\" signs\n",
        "prep_mobydick_tokens = [word for word in mobydick_tokens if word not in string.punctuation and word not in [\"*\", \"<\", \">\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bqsECS6wt5k"
      },
      "source": [
        "Compute the ngram rankings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGzSCG4twt5k",
        "outputId": "938032ba-e0c5-43c4-95c3-c4486058cce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most common bigrams are:\n",
            "[(('of', 'the'), 1895), (('’', 's'), 1790), (('in', 'the'), 1139), (('to', 'the'), 732), (('”', '“'), 494), (('from', 'the'), 437), (('of', 'his'), 371), (('and', 'the'), 364), (('of', 'a'), 337), (('on', 'the'), 337)]\n",
            "\n",
            "The most common trigrams are:\n",
            "[(('don', '’', 't'), 95), (('of', 'the', 'whale'), 88), (('whale', '’', 's'), 80), (('the', 'Sperm', 'Whale'), 77), (('ship', '’', 's'), 77), (('Ahab', '’', 's'), 76), (('’', 's', 'the'), 74), (('he', '’', 's'), 74), (('’', 's', 'a'), 73), (('it', '’', 's'), 63)]\n",
            "\n",
            "The most common fourgrams are:\n",
            "[(('the', 'ship', '’', 's'), 52), (('the', 'whale', '’', 's'), 48), (('the', 'Pequod', '’', 's'), 45), (('I', 'don', '’', 't'), 36), (('of', 'the', 'Sperm', 'Whale'), 30), (('”', 'said', 'I', '“'), 28), (('Sperm', 'Whale', '’', 's'), 24), (('at', 'the', 'same', 'time'), 20), (('old', 'man', '’', 's'), 19), (('of', 'the', 'whale', '’'), 17)]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "bigram_counts = Counter(list(nltk.bigrams(prep_mobydick_tokens)))\n",
        "print(f\"The most common bigrams are:\\n{bigram_counts.most_common(10)}\\n\")\n",
        "trigram_counts = Counter(list(nltk.trigrams(prep_mobydick_tokens)))\n",
        "print(f\"The most common trigrams are:\\n{trigram_counts.most_common(10)}\\n\")\n",
        "fourgram_counts = Counter(nltk.ngrams(prep_mobydick_tokens, 4))\n",
        "print(f\"The most common fourgrams are:\\n{fourgram_counts.most_common(10)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5H5CnBawt5k"
      },
      "source": [
        "## Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "luC7HZ6iwt5l"
      },
      "outputs": [],
      "source": [
        "with open(\"./Adventures_Holmes.txt\", 'r') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from nltk import bigrams, word_tokenize\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "import random\n",
        "\n",
        "# Split the text into paragraphs\n",
        "paragraphs = text.split('\\n\\n')\n",
        "\n",
        "# Select a random paragraph for testing and use the rest for training\n",
        "random_index = random.randint(0, len(paragraphs) - 1)\n",
        "test_paragraph = paragraphs[random_index]\n",
        "training_paragraphs = paragraphs[:random_index] + paragraphs[random_index + 1:]\n"
      ],
      "metadata": {
        "id": "Ud-y6nuGzrrL"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(paragraphs), len(training_paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRbgmjBYz0kt",
        "outputId": "fbf4ed01-cb0c-4dd2-ec83-baac51c44ce6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2610, 2609)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training data\n",
        "training_text = \"\\n\\n\".join(training_paragraphs)\n",
        "training_tokens = word_tokenize(training_text.lower())\n",
        "training_bigrams = list(bigrams(training_tokens))\n",
        "\n",
        "# Build the bigram model\n",
        "model = ConditionalFreqDist(training_bigrams)"
      ],
      "metadata": {
        "id": "VGpclqLS0Zqz"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the next word\n",
        "def predict_next_word(word):\n",
        "    word = word.lower()\n",
        "    if word in model:\n",
        "        return model[word].max()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Tokenize the test paragraph and predict the next word for each token\n",
        "test_tokens = word_tokenize(test_paragraph.lower())\n",
        "predictions = [(word, predict_next_word(word)) for word in test_tokens[:-1]]  # Exclude the last token\n",
        "\n",
        "# Print the predictions\n",
        "for current_word, next_word in predictions:\n",
        "    print(f\"Current word: '{current_word}', Predicted next word: '{next_word}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Lsv1ZV0j6k",
        "outputId": "3101e031-655c-4402-9606-1c7081968043"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current word: '“', Predicted next word: 'i'\n",
            "Current word: 'well', Predicted next word: ','\n",
            "Current word: ',', Predicted next word: 'and'\n",
            "Current word: 'have', Predicted next word: 'been'\n",
            "Current word: 'you', Predicted next word: 'have'\n",
            "Current word: 'solved', Predicted next word: ','\n",
            "Current word: 'it', Predicted next word: 'is'\n",
            "Current word: '?', Predicted next word: '”'\n",
            "Current word: '”', Predicted next word: '“'\n",
            "Current word: 'i', Predicted next word: 'have'\n",
            "Current word: 'asked', Predicted next word: '.'\n",
            "Current word: 'as', Predicted next word: 'i'\n",
            "Current word: 'i', Predicted next word: 'have'\n",
            "Current word: 'entered', Predicted next word: ','\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brj2Q0Rp0xQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7K5Uk0W00xNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLAg5C2hwt5l",
        "outputId": "af4e9563-17f1-4d25-cd7a-152599d93d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2610\n"
          ]
        }
      ],
      "source": [
        "paragraphs = content.split('\\n\\n')\n",
        "print(len(paragraphs))\n",
        "test_paragraph = paragraphs[19]\n",
        "train_paragraphs = paragraphs.pop(19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ESY6QrApwt5l"
      },
      "outputs": [],
      "source": [
        "train_tok = nltk.word_tokenize(train_paragraphs)\n",
        "bigrams_md = list(nltk.bigrams(train_tok))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIibEL2wwt5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh432Es1wt5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiE7n9VLwt5m",
        "outputId": "0ef67273-de96-4bd0-cba2-496aadf481bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({('I', 'could'): 2, ('could', 'not'): 1, ('not', 'help'): 1, ('help', 'laughing'): 1, ('laughing', 'at'): 1, ('at', 'the'): 1, ('the', 'ease'): 1, ('ease', 'with'): 1, ('with', 'which'): 1, ('which', 'he'): 1, ('he', 'explained'): 1, ('explained', 'his'): 1, ('his', 'process'): 1, ('process', 'of'): 1, ('of', 'deduction'): 1, ('deduction', '.'): 1, ('.', '“'): 1, ('“', 'When'): 1, ('When', 'I'): 1, ('I', 'hear'): 1, ('hear', 'you'): 1, ('you', 'give'): 1, ('give', 'your'): 1, ('your', 'reasons'): 1, ('reasons', ','): 1, (',', '”'): 1, ('”', 'I'): 1, ('I', 'remarked'): 1, ('remarked', ','): 1, (',', '“'): 1, ('“', 'the'): 1, ('the', 'thing'): 1, ('thing', 'always'): 1, ('always', 'appears'): 1, ('appears', 'to'): 1, ('to', 'me'): 1, ('me', 'to'): 1, ('to', 'be'): 1, ('be', 'so'): 1, ('so', 'ridiculously'): 1, ('ridiculously', 'simple'): 1, ('simple', 'that'): 1, ('that', 'I'): 1, ('could', 'easily'): 1, ('easily', 'do'): 1, ('do', 'it'): 1, ('it', 'myself'): 1, ('myself', ','): 1, (',', 'though'): 1, ('though', 'at'): 1, ('at', 'each'): 1, ('each', 'successive'): 1, ('successive', 'instance'): 1, ('instance', 'of'): 1, ('of', 'your'): 1, ('your', 'reasoning'): 1, ('reasoning', 'I'): 1, ('I', 'am'): 1, ('am', 'baffled'): 1, ('baffled', 'until'): 1, ('until', 'you'): 1, ('you', 'explain'): 1, ('explain', 'your'): 1, ('your', 'process'): 1, ('process', '.'): 1, ('.', 'And'): 1, ('And', 'yet'): 1, ('yet', 'I'): 1, ('I', 'believe'): 1, ('believe', 'that'): 1, ('that', 'my'): 1, ('my', 'eyes'): 1, ('eyes', 'are'): 1, ('are', 'as'): 1, ('as', 'good'): 1, ('good', 'as'): 1, ('as', 'yours'): 1, ('yours', '.'): 1, ('.', '”'): 1})\n"
          ]
        }
      ],
      "source": [
        "unigram_count =  Counter(list(train_tok))\n",
        "bigram_count = Counter(list(bigrams_md))\n",
        "bigram_prob = dict()\n",
        "print(bigram_count)\n",
        "\n",
        "for tok,freq in bigram_count.items():\n",
        "\n",
        "    bigram_prob[tok] = freq/unigram_count[tok[0]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhJxtFCkwt5n"
      },
      "outputs": [],
      "source": [
        "print(bigram_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jXqdtOfwt5n"
      },
      "outputs": [],
      "source": [
        "test_tok = nltk.word_tokenize(test_paragraph)\n",
        "print(test_tok)\n",
        "predict_dict = dict()\n",
        "for tok in test_tok:\n",
        "    prob = 0\n",
        "    word = \"\"\n",
        "    for bigram1,new_prob in bigram_prob.items():\n",
        "        print(tok)\n",
        "        if tok in list(bigram1):\n",
        "            if new_prob > prob:\n",
        "                prob = new_prob\n",
        "                word = tok\n",
        "    predict_dict[tok] = word\n",
        "\n",
        "print(predict_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQYgvfY8wt5o"
      },
      "source": [
        "## Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqjZiZ47wt5o"
      },
      "source": [
        "### 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYaHgl23wt5o"
      },
      "outputs": [],
      "source": [
        "with open(\".\\contents\\Adventures_Holmes.txt\") as file:\n",
        "    holmes = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH3rJaKzwt5p"
      },
      "source": [
        " ### 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh-4dXYVwt5p"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Get one paragraph of the holmes book as test data, and the rest of paragraphs as train data\n",
        "n_of_paragraphs = len(holmes.split('\\n\\n')[:])\n",
        "test_paragraph = holmes.split('\\n\\n')[18]\n",
        "#train paragraphs should include everything except the test paragraph\n",
        "train_paragraphs = holmes.split('\\n\\n')[:18] + holmes.split('\\n\\n')[18+1:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a88VTrAowt5p"
      },
      "source": [
        "Now let's print how the test paragraph looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "523EJiZ_wt5q"
      },
      "outputs": [],
      "source": [
        "test_paragraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GfTQ_tTwt5q"
      },
      "source": [
        "Now for each word in the test set let's predict the following word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thvFmrzWwt5q"
      },
      "outputs": [],
      "source": [
        "#Now for each word in the test set let's predict the following word\n",
        "test_tok = nltk.word_tokenize(test_paragraph)\n",
        "train_tok = nltk.word_tokenize(train_paragraphs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUW1v98uwt5r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}