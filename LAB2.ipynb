{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\neild\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neild\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Consider the sentence “To Sherlock Holmes she is always ‘The Woman.’ I have seldom heard him mention her under any other name.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"To Sherlock Holmes she is always 'The Woman' I have seldom heard him mention her under any other name.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenize the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " 'she',\n",
       " 'is',\n",
       " 'always',\n",
       " \"'The\",\n",
       " 'Woman',\n",
       " \"'\",\n",
       " 'I',\n",
       " 'have',\n",
       " 'seldom',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'mention',\n",
       " 'her',\n",
       " 'under',\n",
       " 'any',\n",
       " 'other',\n",
       " 'name',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate the n-grams of the sentence, with n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To', 'Sherlock'),\n",
       " ('Sherlock', 'Holmes'),\n",
       " ('Holmes', 'she'),\n",
       " ('she', 'is'),\n",
       " ('is', 'always'),\n",
       " ('always', \"'The\"),\n",
       " (\"'The\", 'Woman'),\n",
       " ('Woman', \"'\"),\n",
       " (\"'\", 'I'),\n",
       " ('I', 'have'),\n",
       " ('have', 'seldom'),\n",
       " ('seldom', 'heard'),\n",
       " ('heard', 'him'),\n",
       " ('him', 'mention'),\n",
       " ('mention', 'her'),\n",
       " ('her', 'under'),\n",
       " ('under', 'any'),\n",
       " ('any', 'other'),\n",
       " ('other', 'name'),\n",
       " ('name', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = list(nltk.bigrams(tokens))\n",
    "bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To', 'Sherlock', 'Holmes'),\n",
       " ('Sherlock', 'Holmes', 'she'),\n",
       " ('Holmes', 'she', 'is'),\n",
       " ('she', 'is', 'always'),\n",
       " ('is', 'always', \"'The\"),\n",
       " ('always', \"'The\", 'Woman'),\n",
       " (\"'The\", 'Woman', \"'\"),\n",
       " ('Woman', \"'\", 'I'),\n",
       " (\"'\", 'I', 'have'),\n",
       " ('I', 'have', 'seldom'),\n",
       " ('have', 'seldom', 'heard'),\n",
       " ('seldom', 'heard', 'him'),\n",
       " ('heard', 'him', 'mention'),\n",
       " ('him', 'mention', 'her'),\n",
       " ('mention', 'her', 'under'),\n",
       " ('her', 'under', 'any'),\n",
       " ('under', 'any', 'other'),\n",
       " ('any', 'other', 'name'),\n",
       " ('other', 'name', '.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = list(nltk.trigrams(tokens))\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove special characters such as *,<,>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', \"'The\", 'Woman', \"'\", 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.']\n"
     ]
    }
   ],
   "source": [
    "#Remove all the tokens that are equal to \"*\", \">\" or \"<\"\n",
    "prep_tok = [word for word in tokens if word not in [\"*\", \">\", \"<\"]]\n",
    "print(prep_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " 'she',\n",
       " 'is',\n",
       " 'always',\n",
       " \"'The\",\n",
       " 'Woman',\n",
       " 'I',\n",
       " 'have',\n",
       " 'seldom',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'mention',\n",
       " 'her',\n",
       " 'under',\n",
       " 'any',\n",
       " 'other',\n",
       " 'name']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove punctuation as well\n",
    "import string\n",
    "prep_tok = [word for word in prep_tok if word not in string.punctuation]\n",
    "prep_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute the frequency of each n-gram, and show the top 10 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter(list(nltk.bigrams(prep_tok)))\n",
    "print(bigram_counts.most_common(10))\n",
    "trigram_counts = Counter(list(nltk.trigrams(prep_tok)))\n",
    "print(trigram_counts.most_common(10))\n",
    "fourgram_counts = Counter(nltk.ngrams(prep_tok, 4))\n",
    "print(fourgram_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's do the same with Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\contents\\moby_dick.txt\") as file:\n",
    "    mobydick = file.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1238254"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobydick_tokens = nltk.word_tokenize(mobydick)\n",
    "len(mobydick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#Now remove punctuations and \"*\", \"<\", \">\" signs\n",
    "prep_mobydick_tokens = [word for word in mobydick_tokens if word not in string.punctuation and word not in [\"*\", \"<\", \">\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the ngram rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common bigrams are:\n",
      "[(('of', 'the'), 1895), (('’', 's'), 1790), (('in', 'the'), 1139), (('to', 'the'), 732), (('”', '“'), 494), (('from', 'the'), 437), (('of', 'his'), 371), (('and', 'the'), 364), (('of', 'a'), 337), (('on', 'the'), 337)]\n",
      "\n",
      "The most common trigrams are:\n",
      "[(('don', '’', 't'), 95), (('of', 'the', 'whale'), 88), (('whale', '’', 's'), 80), (('the', 'Sperm', 'Whale'), 77), (('ship', '’', 's'), 77), (('Ahab', '’', 's'), 76), (('’', 's', 'the'), 74), (('he', '’', 's'), 74), (('’', 's', 'a'), 73), (('it', '’', 's'), 63)]\n",
      "\n",
      "The most common fourgrams are:\n",
      "[(('the', 'ship', '’', 's'), 52), (('the', 'whale', '’', 's'), 48), (('the', 'Pequod', '’', 's'), 45), (('I', 'don', '’', 't'), 36), (('of', 'the', 'Sperm', 'Whale'), 30), (('”', 'said', 'I', '“'), 28), (('Sperm', 'Whale', '’', 's'), 24), (('at', 'the', 'same', 'time'), 20), (('old', 'man', '’', 's'), 19), (('of', 'the', 'whale', '’'), 17)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter(list(nltk.bigrams(prep_mobydick_tokens)))\n",
    "print(f\"The most common bigrams are:\\n{bigram_counts.most_common(10)}\\n\")\n",
    "trigram_counts = Counter(list(nltk.trigrams(prep_mobydick_tokens)))\n",
    "print(f\"The most common trigrams are:\\n{trigram_counts.most_common(10)}\\n\")\n",
    "fourgram_counts = Counter(nltk.ngrams(prep_mobydick_tokens, 4))\n",
    "print(f\"The most common fourgrams are:\\n{fourgram_counts.most_common(10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\contents\\Adventures_Holmes.txt\") as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n"
     ]
    }
   ],
   "source": [
    "paragraphs = content.split('\\n\\n')\n",
    "print(len(paragraphs))\n",
    "test_paragraph = paragraphs[19]\n",
    "train_paragraphs = paragraphs.pop(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'could'),\n",
       " ('could', 'not'),\n",
       " ('not', 'help'),\n",
       " ('help', 'laughing'),\n",
       " ('laughing', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'ease'),\n",
       " ('ease', 'with'),\n",
       " ('with', 'which'),\n",
       " ('which', 'he'),\n",
       " ('he', 'explained'),\n",
       " ('explained', 'his'),\n",
       " ('his', 'process'),\n",
       " ('process', 'of'),\n",
       " ('of', 'deduction'),\n",
       " ('deduction', '.'),\n",
       " ('.', '“'),\n",
       " ('“', 'When'),\n",
       " ('When', 'I'),\n",
       " ('I', 'hear'),\n",
       " ('hear', 'you'),\n",
       " ('you', 'give'),\n",
       " ('give', 'your'),\n",
       " ('your', 'reasons'),\n",
       " ('reasons', ','),\n",
       " (',', '”'),\n",
       " ('”', 'I'),\n",
       " ('I', 'remarked'),\n",
       " ('remarked', ','),\n",
       " (',', '“'),\n",
       " ('“', 'the'),\n",
       " ('the', 'thing'),\n",
       " ('thing', 'always'),\n",
       " ('always', 'appears'),\n",
       " ('appears', 'to'),\n",
       " ('to', 'me'),\n",
       " ('me', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'so'),\n",
       " ('so', 'ridiculously'),\n",
       " ('ridiculously', 'simple'),\n",
       " ('simple', 'that'),\n",
       " ('that', 'I'),\n",
       " ('I', 'could'),\n",
       " ('could', 'easily'),\n",
       " ('easily', 'do'),\n",
       " ('do', 'it'),\n",
       " ('it', 'myself'),\n",
       " ('myself', ','),\n",
       " (',', 'though'),\n",
       " ('though', 'at'),\n",
       " ('at', 'each'),\n",
       " ('each', 'successive'),\n",
       " ('successive', 'instance'),\n",
       " ('instance', 'of'),\n",
       " ('of', 'your'),\n",
       " ('your', 'reasoning'),\n",
       " ('reasoning', 'I'),\n",
       " ('I', 'am'),\n",
       " ('am', 'baffled'),\n",
       " ('baffled', 'until'),\n",
       " ('until', 'you'),\n",
       " ('you', 'explain'),\n",
       " ('explain', 'your'),\n",
       " ('your', 'process'),\n",
       " ('process', '.'),\n",
       " ('.', 'And'),\n",
       " ('And', 'yet'),\n",
       " ('yet', 'I'),\n",
       " ('I', 'believe'),\n",
       " ('believe', 'that'),\n",
       " ('that', 'my'),\n",
       " ('my', 'eyes'),\n",
       " ('eyes', 'are'),\n",
       " ('are', 'as'),\n",
       " ('as', 'good'),\n",
       " ('good', 'as'),\n",
       " ('as', 'yours'),\n",
       " ('yours', '.'),\n",
       " ('.', '”')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_tok = nltk.word_tokenize(train_paragraphs)\n",
    "bigrams_md = list(nltk.bigrams(train_tok))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'I': 6, '.': 3, 'your': 3, ',': 3, 'could': 2, 'at': 2, 'the': 2, 'process': 2, 'of': 2, '“': 2, 'you': 2, '”': 2, 'to': 2, 'that': 2, 'as': 2, 'not': 1, 'help': 1, 'laughing': 1, 'ease': 1, 'with': 1, 'which': 1, 'he': 1, 'explained': 1, 'his': 1, 'deduction': 1, 'When': 1, 'hear': 1, 'give': 1, 'reasons': 1, 'remarked': 1, 'thing': 1, 'always': 1, 'appears': 1, 'me': 1, 'be': 1, 'so': 1, 'ridiculously': 1, 'simple': 1, 'easily': 1, 'do': 1, 'it': 1, 'myself': 1, 'though': 1, 'each': 1, 'successive': 1, 'instance': 1, 'reasoning': 1, 'am': 1, 'baffled': 1, 'until': 1, 'explain': 1, 'And': 1, 'yet': 1, 'believe': 1, 'my': 1, 'eyes': 1, 'are': 1, 'good': 1, 'yours': 1})\n",
      "Counter({('I', 'could'): 2, ('could', 'not'): 1, ('not', 'help'): 1, ('help', 'laughing'): 1, ('laughing', 'at'): 1, ('at', 'the'): 1, ('the', 'ease'): 1, ('ease', 'with'): 1, ('with', 'which'): 1, ('which', 'he'): 1, ('he', 'explained'): 1, ('explained', 'his'): 1, ('his', 'process'): 1, ('process', 'of'): 1, ('of', 'deduction'): 1, ('deduction', '.'): 1, ('.', '“'): 1, ('“', 'When'): 1, ('When', 'I'): 1, ('I', 'hear'): 1, ('hear', 'you'): 1, ('you', 'give'): 1, ('give', 'your'): 1, ('your', 'reasons'): 1, ('reasons', ','): 1, (',', '”'): 1, ('”', 'I'): 1, ('I', 'remarked'): 1, ('remarked', ','): 1, (',', '“'): 1, ('“', 'the'): 1, ('the', 'thing'): 1, ('thing', 'always'): 1, ('always', 'appears'): 1, ('appears', 'to'): 1, ('to', 'me'): 1, ('me', 'to'): 1, ('to', 'be'): 1, ('be', 'so'): 1, ('so', 'ridiculously'): 1, ('ridiculously', 'simple'): 1, ('simple', 'that'): 1, ('that', 'I'): 1, ('could', 'easily'): 1, ('easily', 'do'): 1, ('do', 'it'): 1, ('it', 'myself'): 1, ('myself', ','): 1, (',', 'though'): 1, ('though', 'at'): 1, ('at', 'each'): 1, ('each', 'successive'): 1, ('successive', 'instance'): 1, ('instance', 'of'): 1, ('of', 'your'): 1, ('your', 'reasoning'): 1, ('reasoning', 'I'): 1, ('I', 'am'): 1, ('am', 'baffled'): 1, ('baffled', 'until'): 1, ('until', 'you'): 1, ('you', 'explain'): 1, ('explain', 'your'): 1, ('your', 'process'): 1, ('process', '.'): 1, ('.', 'And'): 1, ('And', 'yet'): 1, ('yet', 'I'): 1, ('I', 'believe'): 1, ('believe', 'that'): 1, ('that', 'my'): 1, ('my', 'eyes'): 1, ('eyes', 'are'): 1, ('are', 'as'): 1, ('as', 'good'): 1, ('good', 'as'): 1, ('as', 'yours'): 1, ('yours', '.'): 1, ('.', '”'): 1})\n"
     ]
    }
   ],
   "source": [
    "unigram_count =  Counter(list(train_tok))\n",
    "bigram_count = Counter(list(bigrams_md))\n",
    "print(bigram_count)\n",
    "\n",
    "[i for i in train_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tok = nltk.word_tokenize(test_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\contents\\Adventures_Holmes.txt\") as file:\n",
    "    holmes = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
